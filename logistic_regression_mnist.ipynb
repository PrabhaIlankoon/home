{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras - Classifying MNIST dataset with Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From Wikipedia, the free encyclopedia (https://en.wikipedia.org/wiki/MNIST_database)\n",
    "\n",
    "The MNIST database (Modified National Institute of Standards and Technology database) is a large database of handwritten digits that is commonly used for training various image processing systems.The database is also widely used for training and testing in the field of machine learning. It was created by \"re-mixing\" the samples from NIST's original datasets. The creators felt that since NIST's training dataset was taken from American Census Bureau employees, while the testing dataset was taken from American high school students, it was not well-suited for machine learning experiments. Furthermore, the black and white images from NIST were normalized to fit into a 28x28 pixel bounding box and anti-aliased, which introduced grayscale levels.\n",
    "\n",
    "The MNIST database contains 60,000 training images and 10,000 testing images. Half of the training set and half of the test set were taken from NIST's training dataset, while the other half of the training set and the other half of the test set were taken from NIST's testing dataset. There have been a number of scientific papers on attempts to achieve the lowest error rate; one paper, using a hierarchical system of convolutional neural networks, manages to get an error rate on the MNIST database of 0.23 percent. The original creators of the database keep a list of some of the methods tested on it. In their original paper, they use a support vector machine to get an error rate of 0.8 percent.An extended dataset similar to MNIST called EMNIST has been published in 2017, which contains 240,000 training images, and 40,000 testing images of handwritten digits.\n",
    "\n",
    "\n",
    "THE MNIST DATABASE of handwritten digits - http://yann.lecun.com/exdb/mnist/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape =  (60000, 28, 28) y_train shape =  (60000,)\n",
      "x_test shape =  (10000, 28, 28) y_test shape =  (10000,)\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist #importing the MNIST dataset\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "print \"x_train shape = \", x_train.shape, \"y_train shape = \", y_train.shape\n",
    "print \"x_test shape = \", x_test.shape, \"y_test shape = \", y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7faab2512490>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAADn9JREFUeJzt3X9sXfV5x/HPU8dxlhDauCmeSzMSIC3QsIbtKoCIgImR\npQgpoKqhUVWljDVdC3RsmQTLpjWb2JRNLVXKGJJZsyQVv0oLIn+wVmBV0GrgYbIQfpVfwV0TjE1w\nIYHSxLGf/eGTygXf73XuPfeeaz/vl2T53vOcc8+jk3x87r3fe8/X3F0A4vlA0Q0AKAbhB4Ii/EBQ\nhB8IivADQRF+ICjCDwRF+IGgCD8Q1IxG7mymtfkszWnkLoFQfq13dNgP2WTWrSn8ZrZS0mZJLZL+\nw903pdafpTk62y6qZZcAEnq8e9LrVv2038xaJN0i6dOSzpC0xszOqPbxADRWLa/5l0l6yd33uPth\nSXdJWpVPWwDqrZbwnyjpF+Pu782W/RYzW2dmvWbWO6xDNewOQJ7q/m6/u3e5e8ndS61qq/fuAExS\nLeHfJ2nBuPsfy5YBmAJqCf/jkhab2SIzmynpc5J25NMWgHqreqjP3Y+Y2TWSfqSxob4t7v5Mbp0B\nqKuaxvnd/QFJD+TUC4AG4uO9QFCEHwiK8ANBEX4gKMIPBEX4gaAIPxAU4QeCIvxAUIQfCIrwA0ER\nfiAowg8ERfiBoAg/EBThB4Ii/EBQhB8IivADQRF+ICjCDwRF+IGgCD8QFOEHgiL8QFCEHwiK8ANB\nEX4gKMIPBFXTLL1m1ifpoKQRSUfcvZRHU8iPzUj/E7d8ZH5d9//8Xy8sWxuZPZrc9qRTBpP12V+1\nZP21m2aWre0s3Z3cdv/IO8n62fesT9ZP/avHkvVmUFP4M3/k7vtzeBwADcTTfiCoWsPvkh4ysyfM\nbF0eDQFojFqf9i93931mdoKkB83sZ+7+yPgVsj8K6yRplmbXuDsAeanpzO/u+7Lfg5Luk7RsgnW6\n3L3k7qVWtdWyOwA5qjr8ZjbHzOYevS1phaSn82oMQH3V8rS/Q9J9Znb0ce5w9x/m0hWAuqs6/O6+\nR9Kncuxl2mo5fXGy7m2tyfqrF3woWX/3nPJj0u0fTI9X/+RT6fHuIv3Xr+Ym6//ybyuT9Z4z7yhb\ne2X43eS2mwYuTtY/+hNP1qcChvqAoAg/EBThB4Ii/EBQhB8IivADQeXxrb7wRi78g2T9pq23JOsf\nby3/1dPpbNhHkvW/v/mLyfqMd9LDbefec03Z2tx9R5Lbtu1PDwXO7u1J1qcCzvxAUIQfCIrwA0ER\nfiAowg8ERfiBoAg/EBTj/Dloe/7VZP2JXy9I1j/eOpBnO7la339Osr7n7fSlv7ee8v2ytbdG0+P0\nHd/+72S9nqb+F3Yr48wPBEX4gaAIPxAU4QeCIvxAUIQfCIrwA0GZe+NGNI+3dj/bLmrY/prF0JXn\nJusHVqYvr92y+7hk/cmv3nzMPR114/7fT9YfvyA9jj/y5lvJup9b/urufV9LbqpFa55Mr4D36fFu\nHfCh9NzlGc78QFCEHwiK8ANBEX4gKMIPBEX4gaAIPxBUxXF+M9si6VJJg+6+JFvWLuluSQsl9Ula\n7e6/rLSzqOP8lbTM/3CyPvLGULL+yh3lx+qfOX9Lcttl/3xtsn7CLcV9px7HLu9x/q2S3jsR+g2S\nut19saTu7D6AKaRi+N39EUnvPfWskrQtu71N0mU59wWgzqp9zd/h7v3Z7dckdeTUD4AGqfkNPx97\n06DsGwdmts7Mes2sd1iHat0dgJxUG/4BM+uUpOz3YLkV3b3L3UvuXmpVW5W7A5C3asO/Q9La7PZa\nSffn0w6ARqkYfjO7U9Kjkj5hZnvN7CpJmyRdbGYvSvrj7D6AKaTidfvdfU2ZEgP2ORnZ/0ZN2w8f\nmFn1tp/8/LPJ+uu3tqQfYHSk6n2jWHzCDwiK8ANBEX4gKMIPBEX4gaAIPxAUU3RPA6df/0LZ2pVn\npkdk//Ok7mT9gs9enazPvfuxZB3NizM/EBThB4Ii/EBQhB8IivADQRF+ICjCDwTFOP80kJom+42v\nnJ7c9v92vJus33Dj9mT9b1Zfnqz7/36wbG3BPz2a3FYNnD4+Is78QFCEHwiK8ANBEX4gKMIPBEX4\ngaAIPxBUxSm688QU3c1n6E/PTdZv//o3kvVFM2ZVve9Pbr8mWV98W3+yfmRPX9X7nq7ynqIbwDRE\n+IGgCD8QFOEHgiL8QFCEHwiK8ANBVRznN7Mtki6VNOjuS7JlGyV9SdLr2Wob3P2BSjtjnH/q8fOW\nJuvHb9qbrN958o+q3vdpP/6zZP0T/1D+OgaSNPLinqr3PVXlPc6/VdLKCZZ/y92XZj8Vgw+guVQM\nv7s/ImmoAb0AaKBaXvNfa2a7zWyLmc3LrSMADVFt+G+VdLKkpZL6JX2z3Ipmts7Mes2sd1iHqtwd\ngLxVFX53H3D3EXcflXSbpGWJdbvcveTupVa1VdsngJxVFX4z6xx393JJT+fTDoBGqXjpbjO7U9KF\nkuab2V5JX5d0oZktleSS+iR9uY49AqgDvs+PmrR0nJCsv3rFqWVrPddvTm77gQpPTD//yopk/a3l\nbyTr0xHf5wdQEeEHgiL8QFCEHwiK8ANBEX4gKIb6UJjv7U1P0T3bZibrv/LDyfql115X/rHv60lu\nO1Ux1AegIsIPBEX4gaAIPxAU4QeCIvxAUIQfCKri9/kR2+jy9KW7X/5seoruJUv7ytYqjeNXcvPQ\nWcn67Pt7a3r86Y4zPxAU4QeCIvxAUIQfCIrwA0ERfiAowg8ExTj/NGelJcn6C19Lj7Xfdt62ZP38\nWenv1NfikA8n648NLUo/wGh/jt1MP5z5gaAIPxAU4QeCIvxAUIQfCIrwA0ERfiCoiuP8ZrZA0nZJ\nHZJcUpe7bzazdkl3S1ooqU/Sanf/Zf1ajWvGopOS9Zev/GjZ2sYr7kpu+5nj9lfVUx42DJSS9Yc3\nn5Osz9uWvu4/0iZz5j8iab27nyHpHElXm9kZkm6Q1O3uiyV1Z/cBTBEVw+/u/e6+M7t9UNJzkk6U\ntErS0Y9/bZN0Wb2aBJC/Y3rNb2YLJZ0lqUdSh7sf/fzkaxp7WQBgiph0+M3sOEk/kHSdux8YX/Ox\nCf8mnPTPzNaZWa+Z9Q7rUE3NAsjPpMJvZq0aC/7t7n5vtnjAzDqzeqekwYm2dfcudy+5e6lVbXn0\nDCAHFcNvZibpO5Kec/ebxpV2SFqb3V4r6f782wNQL5P5Su95kr4g6Skz25Ut2yBpk6TvmdlVkn4u\naXV9Wpz6Ziz8vWT9rT/sTNav+McfJut//qF7k/V6Wt+fHo579N/LD+e1b/2f5LbzRhnKq6eK4Xf3\nn0oqN9/3Rfm2A6BR+IQfEBThB4Ii/EBQhB8IivADQRF+ICgu3T1JMzp/t2xtaMuc5LZfWfRwsr5m\n7kBVPeXhmn3Lk/Wdt6an6J7//aeT9faDjNU3K878QFCEHwiK8ANBEX4gKMIPBEX4gaAIPxBUmHH+\nw3+Svkz04b8cStY3nPpA2dqK33mnqp7yMjDybtna+TvWJ7c97e9+lqy3v5kepx9NVtHMOPMDQRF+\nICjCDwRF+IGgCD8QFOEHgiL8QFBhxvn7Lkv/nXvhzHvqtu9b3jwlWd/88Ipk3UbKXTl9zGk3vlK2\ntnigJ7ntSLKK6YwzPxAU4QeCIvxAUIQfCIrwA0ERfiAowg8EZe6eXsFsgaTtkjokuaQud99sZhsl\nfUnS69mqG9y9/JfeJR1v7X62Mas3UC893q0DPpT+YEhmMh/yOSJpvbvvNLO5kp4wswez2rfc/RvV\nNgqgOBXD7+79kvqz2wfN7DlJJ9a7MQD1dUyv+c1soaSzJB39zOi1ZrbbzLaY2bwy26wzs14z6x3W\noZqaBZCfSYffzI6T9ANJ17n7AUm3SjpZ0lKNPTP45kTbuXuXu5fcvdSqthxaBpCHSYXfzFo1Fvzb\n3f1eSXL3AXcfcfdRSbdJWla/NgHkrWL4zcwkfUfSc+5+07jlneNWu1xSerpWAE1lMu/2nyfpC5Ke\nMrNd2bINktaY2VKNDf/1SfpyXToEUBeTebf/p5ImGjdMjukDaG58wg8IivADQRF+ICjCDwRF+IGg\nCD8QFOEHgiL8QFCEHwiK8ANBEX4gKMIPBEX4gaAIPxBUxUt357ozs9cl/XzcovmS9jesgWPTrL01\na18SvVUrz95OcvePTGbFhob/fTs363X3UmENJDRrb83al0Rv1SqqN572A0ERfiCoosPfVfD+U5q1\nt2btS6K3ahXSW6Gv+QEUp+gzP4CCFBJ+M1tpZs+b2UtmdkMRPZRjZn1m9pSZ7TKz3oJ72WJmg2b2\n9Lhl7Wb2oJm9mP2ecJq0gnrbaGb7smO3y8wuKai3BWb2YzN71syeMbO/yJYXeuwSfRVy3Br+tN/M\nWiS9IOliSXslPS5pjbs/29BGyjCzPkkldy98TNjMzpf0tqTt7r4kW/avkobcfVP2h3Oeu1/fJL1t\nlPR20TM3ZxPKdI6fWVrSZZK+qAKPXaKv1SrguBVx5l8m6SV33+PuhyXdJWlVAX00PXd/RNLQexav\nkrQtu71NY/95Gq5Mb03B3fvdfWd2+6CkozNLF3rsEn0VoojwnyjpF+Pu71VzTfntkh4ysyfMbF3R\nzUygI5s2XZJek9RRZDMTqDhzcyO9Z2bppjl21cx4nTfe8Hu/5e6+VNKnJV2dPb1tSj72mq2Zhmsm\nNXNzo0wws/RvFHnsqp3xOm9FhH+fpAXj7n8sW9YU3H1f9ntQ0n1qvtmHB45Okpr9Hiy4n99oppmb\nJ5pZWk1w7Jppxusiwv+4pMVmtsjMZkr6nKQdBfTxPmY2J3sjRmY2R9IKNd/swzskrc1ur5V0f4G9\n/JZmmbm53MzSKvjYNd2M1+7e8B9Jl2jsHf+XJf1tET2U6etkSU9mP88U3ZukOzX2NHBYY++NXCXp\nw5K6Jb0o6SFJ7U3U23clPSVpt8aC1llQb8s19pR+t6Rd2c8lRR+7RF+FHDc+4QcExRt+QFCEHwiK\n8ANBEX4gKMIPBEX4gaAIPxAU4QeC+n8DZI6NXofNrQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7faab2907f50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.imshow(x_train[0]) #plots the first digit stored in the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7faab243f1d0>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7faab2532890>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(x_test[0]) #plots the first digit stored in the testing dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Casting inputs to float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train = x_train.astype('float32')\n",
    "x_test  = x_test.astype('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reshaping inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28) => (60000, 784)\n",
      "(10000, 28, 28) => (10000, 784)\n"
     ]
    }
   ],
   "source": [
    "INPUT_DIM = 784 #28 by 28\n",
    "\n",
    "x_train_reshape = x_train.reshape(60000, INPUT_DIM)\n",
    "x_test_reshape = x_test.reshape(10000, INPUT_DIM)\n",
    "print x_train.shape, \"=>\", x_train_reshape.shape\n",
    "print x_test.shape,  \"=>\", x_test_reshape.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalizing the inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train_reshape /= 255\n",
    "x_test_reshape  /= 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting labels to one-hot vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension of y_train_one_hot =  (60000, 10)\n",
      "Dimension of y_test_one_hot  =  (10000, 10)\n",
      "5 => [ 0.  0.  0.  0.  0.  1.  0.  0.  0.  0.]\n",
      "7 => [ 0.  0.  0.  0.  0.  0.  0.  1.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import np_utils\n",
    "\n",
    "NB_CLASSES = 10 #nb_classes = 10 (number of classes)\n",
    "y_train_one_hot = np_utils.to_categorical(y_train, NB_CLASSES) \n",
    "y_test_one_hot  = np_utils.to_categorical(y_test, NB_CLASSES)\n",
    "print \"Dimension of y_train_one_hot = \", y_train_one_hot.shape\n",
    "print \"Dimension of y_test_one_hot  = \", y_test_one_hot.shape\n",
    "print y_train[0], \"=>\", y_train_one_hot[0]\n",
    "print y_test[0],  \"=>\", y_test_one_hot[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the logistic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_5 (Dense)              (None, 10)                7850      \n",
      "=================================================================\n",
      "Total params: 7,850\n",
      "Trainable params: 7,850\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(NB_CLASSES, activation='softmax', input_shape=(INPUT_DIM,)))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compiling the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.optimizers import SGD\n",
    "\n",
    "OPTIMIZER = SGD(lr=0.1)\n",
    "model.compile(optimizer=OPTIMIZER, loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/50\n",
      "48000/48000 [==============================] - 0s - loss: 0.6267 - acc: 0.8437 - val_loss: 0.3952 - val_acc: 0.8978\n",
      "Epoch 2/50\n",
      "48000/48000 [==============================] - 0s - loss: 0.3954 - acc: 0.8924 - val_loss: 0.3484 - val_acc: 0.9035\n",
      "Epoch 3/50\n",
      "48000/48000 [==============================] - 1s - loss: 0.3587 - acc: 0.9010 - val_loss: 0.3268 - val_acc: 0.9116\n",
      "Epoch 4/50\n",
      "48000/48000 [==============================] - 0s - loss: 0.3400 - acc: 0.9049 - val_loss: 0.3140 - val_acc: 0.9137\n",
      "Epoch 5/50\n",
      "48000/48000 [==============================] - 0s - loss: 0.3276 - acc: 0.9086 - val_loss: 0.3072 - val_acc: 0.9156\n",
      "Epoch 6/50\n",
      "48000/48000 [==============================] - 1s - loss: 0.3194 - acc: 0.9106 - val_loss: 0.3006 - val_acc: 0.9163\n",
      "Epoch 7/50\n",
      "48000/48000 [==============================] - 0s - loss: 0.3125 - acc: 0.9131 - val_loss: 0.2971 - val_acc: 0.9170\n",
      "Epoch 8/50\n",
      "48000/48000 [==============================] - 0s - loss: 0.3073 - acc: 0.9144 - val_loss: 0.2934 - val_acc: 0.9174\n",
      "Epoch 9/50\n",
      "48000/48000 [==============================] - 1s - loss: 0.3027 - acc: 0.9157 - val_loss: 0.2905 - val_acc: 0.9192\n",
      "Epoch 10/50\n",
      "48000/48000 [==============================] - 1s - loss: 0.2990 - acc: 0.9162 - val_loss: 0.2875 - val_acc: 0.9199\n",
      "Epoch 11/50\n",
      "48000/48000 [==============================] - 0s - loss: 0.2962 - acc: 0.9172 - val_loss: 0.2851 - val_acc: 0.9203\n",
      "Epoch 12/50\n",
      "48000/48000 [==============================] - 1s - loss: 0.2930 - acc: 0.9180 - val_loss: 0.2842 - val_acc: 0.9193\n",
      "Epoch 13/50\n",
      "48000/48000 [==============================] - 0s - loss: 0.2909 - acc: 0.9181 - val_loss: 0.2827 - val_acc: 0.9207\n",
      "Epoch 14/50\n",
      "48000/48000 [==============================] - 1s - loss: 0.2886 - acc: 0.9190 - val_loss: 0.2818 - val_acc: 0.9211\n",
      "Epoch 15/50\n",
      "48000/48000 [==============================] - 1s - loss: 0.2864 - acc: 0.9199 - val_loss: 0.2806 - val_acc: 0.9225\n",
      "Epoch 16/50\n",
      "48000/48000 [==============================] - 1s - loss: 0.2848 - acc: 0.9206 - val_loss: 0.2781 - val_acc: 0.9223\n",
      "Epoch 17/50\n",
      "48000/48000 [==============================] - 1s - loss: 0.2830 - acc: 0.9211 - val_loss: 0.2786 - val_acc: 0.9216\n",
      "Epoch 18/50\n",
      "48000/48000 [==============================] - 1s - loss: 0.2817 - acc: 0.9216 - val_loss: 0.2774 - val_acc: 0.9222\n",
      "Epoch 19/50\n",
      "48000/48000 [==============================] - 1s - loss: 0.2802 - acc: 0.9215 - val_loss: 0.2760 - val_acc: 0.9230\n",
      "Epoch 20/50\n",
      "48000/48000 [==============================] - 1s - loss: 0.2789 - acc: 0.9218 - val_loss: 0.2750 - val_acc: 0.9242\n",
      "Epoch 21/50\n",
      "48000/48000 [==============================] - 1s - loss: 0.2776 - acc: 0.9229 - val_loss: 0.2748 - val_acc: 0.9235\n",
      "Epoch 22/50\n",
      "48000/48000 [==============================] - 1s - loss: 0.2767 - acc: 0.9231 - val_loss: 0.2739 - val_acc: 0.9231\n",
      "Epoch 23/50\n",
      "48000/48000 [==============================] - 1s - loss: 0.2755 - acc: 0.9231 - val_loss: 0.2736 - val_acc: 0.9239\n",
      "Epoch 24/50\n",
      "48000/48000 [==============================] - 1s - loss: 0.2744 - acc: 0.9235 - val_loss: 0.2733 - val_acc: 0.9241\n",
      "Epoch 25/50\n",
      "48000/48000 [==============================] - 1s - loss: 0.2735 - acc: 0.9244 - val_loss: 0.2750 - val_acc: 0.9234\n",
      "Epoch 26/50\n",
      "48000/48000 [==============================] - 1s - loss: 0.2725 - acc: 0.9236 - val_loss: 0.2728 - val_acc: 0.9246\n",
      "Epoch 27/50\n",
      "48000/48000 [==============================] - 1s - loss: 0.2717 - acc: 0.9241 - val_loss: 0.2730 - val_acc: 0.9241\n",
      "Epoch 28/50\n",
      "48000/48000 [==============================] - 1s - loss: 0.2711 - acc: 0.9243 - val_loss: 0.2728 - val_acc: 0.9248\n",
      "Epoch 29/50\n",
      "48000/48000 [==============================] - 1s - loss: 0.2702 - acc: 0.9250 - val_loss: 0.2710 - val_acc: 0.9247\n",
      "Epoch 30/50\n",
      "48000/48000 [==============================] - 1s - loss: 0.2697 - acc: 0.9250 - val_loss: 0.2706 - val_acc: 0.9253\n",
      "Epoch 31/50\n",
      "48000/48000 [==============================] - 1s - loss: 0.2688 - acc: 0.9251 - val_loss: 0.2706 - val_acc: 0.9256\n",
      "Epoch 32/50\n",
      "48000/48000 [==============================] - 1s - loss: 0.2680 - acc: 0.9252 - val_loss: 0.2707 - val_acc: 0.9244\n",
      "Epoch 33/50\n",
      "48000/48000 [==============================] - 1s - loss: 0.2676 - acc: 0.9252 - val_loss: 0.2704 - val_acc: 0.9247\n",
      "Epoch 34/50\n",
      "48000/48000 [==============================] - 1s - loss: 0.2670 - acc: 0.9252 - val_loss: 0.2693 - val_acc: 0.9253\n",
      "Epoch 35/50\n",
      "48000/48000 [==============================] - 1s - loss: 0.2662 - acc: 0.9255 - val_loss: 0.2705 - val_acc: 0.9252\n",
      "Epoch 36/50\n",
      "48000/48000 [==============================] - 1s - loss: 0.2657 - acc: 0.9261 - val_loss: 0.2696 - val_acc: 0.9253\n",
      "Epoch 37/50\n",
      "48000/48000 [==============================] - 1s - loss: 0.2650 - acc: 0.9261 - val_loss: 0.2697 - val_acc: 0.9265\n",
      "Epoch 38/50\n",
      "48000/48000 [==============================] - 1s - loss: 0.2646 - acc: 0.9260 - val_loss: 0.2685 - val_acc: 0.9258\n",
      "Epoch 39/50\n",
      "48000/48000 [==============================] - 1s - loss: 0.2640 - acc: 0.9265 - val_loss: 0.2690 - val_acc: 0.9250\n",
      "Epoch 40/50\n",
      "48000/48000 [==============================] - 1s - loss: 0.2635 - acc: 0.9267 - val_loss: 0.2680 - val_acc: 0.9268\n",
      "Epoch 41/50\n",
      "48000/48000 [==============================] - 1s - loss: 0.2628 - acc: 0.9270 - val_loss: 0.2691 - val_acc: 0.9260\n",
      "Epoch 42/50\n",
      "48000/48000 [==============================] - 1s - loss: 0.2625 - acc: 0.9269 - val_loss: 0.2693 - val_acc: 0.9263\n",
      "Epoch 43/50\n",
      "48000/48000 [==============================] - 1s - loss: 0.2621 - acc: 0.9268 - val_loss: 0.2683 - val_acc: 0.9260\n",
      "Epoch 44/50\n",
      "48000/48000 [==============================] - 1s - loss: 0.2617 - acc: 0.9269 - val_loss: 0.2691 - val_acc: 0.9263\n",
      "Epoch 45/50\n",
      "48000/48000 [==============================] - 1s - loss: 0.2613 - acc: 0.9270 - val_loss: 0.2676 - val_acc: 0.9258\n",
      "Epoch 46/50\n",
      "48000/48000 [==============================] - 1s - loss: 0.2609 - acc: 0.9277 - val_loss: 0.2676 - val_acc: 0.9265\n",
      "Epoch 47/50\n",
      "48000/48000 [==============================] - 1s - loss: 0.2603 - acc: 0.9272 - val_loss: 0.2678 - val_acc: 0.9264\n",
      "Epoch 48/50\n",
      "48000/48000 [==============================] - 1s - loss: 0.2601 - acc: 0.9278 - val_loss: 0.2673 - val_acc: 0.9261\n",
      "Epoch 49/50\n",
      "48000/48000 [==============================] - 1s - loss: 0.2596 - acc: 0.9274 - val_loss: 0.2665 - val_acc: 0.9272\n",
      "Epoch 50/50\n",
      "48000/48000 [==============================] - 1s - loss: 0.2593 - acc: 0.9281 - val_loss: 0.2676 - val_acc: 0.9268\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 128\n",
    "EPOCHES = 50\n",
    "VERBOSE = 1\n",
    "VALIDATION_SPLIT = 0.2\n",
    "\n",
    "model_history = model.fit(x=x_train_reshape, y=y_train_one_hot, batch_size=BATCH_SIZE, \n",
    "                          epochs=EPOCHES, verbose=VERBOSE, validation_split=VALIDATION_SPLIT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Test score:', 0.27096405602395535)\n",
      "('Test accuracy:', 0.92210000000000003)\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(x_test_reshape, y_test_one_hot, verbose=0)\n",
    "print('Test score:', score[0]) \n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting the class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True class =  7 Predicted class =  7\n"
     ]
    }
   ],
   "source": [
    "y_probability = model.predict(x_test_reshape)\n",
    "y_classes = y_probability.argmax(axis=-1)\n",
    "print \"True class = \", y_test[0], \"Predicted class = \", y_classes[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the trained model and its weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "json_string = model.to_json() # as json \n",
    "open('mnist_Logistic_model.json', 'w').write(json_string)\n",
    "# save the weights in h5 format \n",
    "model.save_weights('mnist_Logistic_wts.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieving a trained model and its weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_5 (Dense)              (None, 10)                7850      \n",
      "=================================================================\n",
      "Total params: 7,850\n",
      "Trainable params: 7,850\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "('Test score:', 0.27096405602395535)\n",
      "('Test accuracy:', 0.92210000000000003)\n",
      "True class =  7 Predicted class =  7\n"
     ]
    }
   ],
   "source": [
    "from keras.models import model_from_json\n",
    "model1 = model_from_json(open('mnist_Logistic_model.json').read())\n",
    "model1.load_weights('mnist_Logistic_wts.h5')\n",
    "model1.summary()\n",
    "model1.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "score = model1.evaluate(x_test_reshape, y_test_one_hot, verbose=0) \n",
    "print('Test score:', score[0]) \n",
    "print('Test accuracy:', score[1])\n",
    "\n",
    "y_probability = model.predict(x_test_reshape)\n",
    "y_classes = y_probability.argmax(axis=-1)\n",
    "print \"True class = \", y_test[0], \"Predicted class = \", y_classes[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
